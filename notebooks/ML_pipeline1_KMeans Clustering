# Unsupervised (rel std, rel se) data  ---> supervised training dataset categorization based on KMeans Clustering vs Manual Reliability classification (Data Classification/Categorization) based on non-ML thresholding   

# Function to convert KMeans based numerical clustering label categorization to reliability classification

def classify_cluster_center(rel_std, rel_se, std_thresh, se_thresh):
    if rel_std <= std_thresh and rel_se <= se_thresh:
        return 'High reliability'
    elif rel_std <= std_thresh and rel_se > se_thresh:
        return 'Medium reliability (low variability, high uncertainty)'
    elif rel_std > std_thresh and rel_se <= se_thresh:
        return 'Medium reliability (high variability, low uncertainty)'
    else:
        return 'Low reliability'



from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from pandas.api.types import CategoricalDtype

fig, axs= plt.subplots(nrows=3, ncols=3, figsize=(30, 20))
fig_manual, axs_manual = plt.subplots(nrows=3, ncols=3, figsize=(30, 20))
axs = axs.flatten()
axs_manual = axs_manual.flatten()

results_per_param = {}
manual_results_per_param = {}
for i, param in enumerate(params):
    rel_std_and_se_data_scatter = compute_rel_std_and_se_data(filtered_df_for_scatterplots, param) # calling the compute_rel_data() function to filter filtered dataframe from flagging operations to the final dataframe of rel_std values used in the box plots
    rel_std_and_se_data_scatter = custom_downsample(rel_std_and_se_data_scatter, sampling_rates=sampling_rates)

    if rel_std_and_se_data_scatter.empty:
        continue  # Skip parameters with no valid data

    # Explicitly drop rows where rel_std or rel_se are NaN after compute step
    rel_std_and_se_data_ml = rel_std_and_se_data_scatter.dropna(subset=['rel_std', 'rel_se']).copy()

    # Standardize rel_std and rel_se
    scaler = StandardScaler()
    rel_std_and_se_data_ml[['rel_std_scaled', 'rel_se_scaled']] = scaler.fit_transform(rel_std_and_se_data_ml[['rel_std', 'rel_se']])

    # Encode 'method' numerical for later use in classification using the RandomForestClassifier
    le = LabelEncoder()
    rel_std_and_se_data_ml['method_encoded'] = le.fit_transform(rel_std_and_se_data_ml['method'])

    # Clustering (rel_std, rel_se) data into 4 clusters : High Reliability (Low Variability, Low Uncertainty), Medium Reliability [(Low Variability, High Uncertainty), ((High Variability, Low Uncertainty)], Low Reliability (High Variability, High Uncertainty)
    kmeans = KMeans(n_clusters=10, random_state=42)

    # Model fitting through clustering and stored in the cluster column
    rel_std_and_se_data_ml['cluster'] = kmeans.fit_predict(rel_std_and_se_data_ml[['rel_std_scaled', 'rel_se_scaled']])


    # Optional: Evaluate cluster separation
    silhouette = silhouette_score(rel_std_and_se_data_ml[['rel_std_scaled', 'rel_se_scaled']], rel_std_and_se_data_ml['cluster'])
    print(f"Silhouette Score: {silhouette:.2f}")

    # Get number of unique clusters
    #num_clusters = rel_std_and_se_data_ml['cluster'].nunique()
    #count_cluster = rel_std_and_se_data_ml['cluster'].value_counts()
    #print(f"{method} – {param}: {count_cluster} values")


    # Build a palette mapping each cluster integer to a color
    #cluster_palette = dict(zip(range(num_clusters), sns.color_palette("Paired", n_colors=num_clusters)))

    # Inverse transform cluster centers to get rel_std and rel_se
    centers_original = scaler.inverse_transform(kmeans.cluster_centers_)

    # Define thresholds — use median or domain-informed values
    #std_thresh = rel_std_and_se_data_ml['rel_std'].median()
    #se_thresh = rel_std_and_se_data_ml['rel_se'].median()

    std_thresh = np.median(centers_original[:, 0])  # rel_std
    se_thresh = np.median(centers_original[:, 1])  # rel_se


    # Assign reliability categories
    cluster_labels = []
    for center in centers_original:
        rel_std, rel_se = center
        label = classify_cluster_center(rel_std, rel_se, std_thresh, se_thresh)
        cluster_labels.append(label)

    # Dictionary mapping numerical cluster to reliability label
    cluster_to_reliability = dict(zip(range(len(cluster_labels)), cluster_labels))

    # Assign to your dataframe
    rel_std_and_se_data_ml['reliability_category'] = rel_std_and_se_data_ml['cluster'].map(cluster_to_reliability)

    if rel_std_and_se_data_ml.empty:
      continue

    #for cluster_label in sorted(rel_std_and_se_data_ml['reliability_category'].unique()):
    #  count = len(rel_std_and_se_data_ml[rel_std_and_se_data_ml['reliability_category'] == cluster_label])
    #  print(f"Cluster {cluster_label}: {count} values")

      # See distribution of methods per cluster
    #  cluster_method_counts = rel_std_and_se_data_ml.groupby('reliability_category')['method'].value_counts(normalize=True)
    #  print(cluster_method_counts)

    # Group once to avoid repeated computation
    grouped = rel_std_and_se_data_ml.groupby('reliability_category')

    for reliability_label, group_df in grouped:
      count = len(group_df)
      print(f"\nParameter: {param} – Cluster: '{reliability_label}' → Total count: {count}")

      method_counts = group_df['method'].value_counts(normalize=True).sort_values(ascending=False)
      print("Method distribution within cluster:")
      for method, pct in method_counts.items():
        print(f"  {method}: {pct:.2%}")


    # First group entire dataset by method and reliability. unstack() is used here which takes the second level of grouped index (i.e. reliability_category entries) and converts it to columns while maintaining method as the row index
    method_reliability_counts = rel_std_and_se_data_ml.groupby(['method', 'reliability_category']).size().unstack(fill_value=0)


    # Ensure all expected reliability categories are present
    for label in rel_std_and_se_data_ml['reliability_category']:
      if label not in method_reliability_counts.columns:
        method_reliability_counts[label] = 0  # add missing columns if needed

    # Print ratio info
    print(f"\nParameter: {param} – Method-wise reliability ratios:")
    for method, row in method_reliability_counts.iterrows():
        high = row.get('High reliability', 0)
        low = row.get('Low reliability', 0)
        medium = row.get('Medium reliability (low variability, high uncertainty)', 0) + \
             row.get('Medium reliability (high variability, low uncertainty)', 0)

        if high > 0:
            low_to_high = low / high
            med_to_high = medium / high
            print(f"  {method}: Low/High = {low_to_high:.4f}, Medium/High = {med_to_high:.4f}")
        else:
            print(f"  {method}: High reliability count is 0 → ratios undefined")

    reliability_palette = {
    'High reliability': '#2ca02c',  # green
    'Medium reliability (low variability, high uncertainty)': '#1f77b4',  # blue
    'Medium reliability (high variability, low uncertainty)': '#ff7f0e',  # orange
    'Low reliability': '#d62728'  # red
}

    all_categories = list(reliability_palette.keys())
    rel_std_and_se_data_ml['reliability_category'] = rel_std_and_se_data_ml['reliability_category'].astype(
    CategoricalDtype(categories=all_categories, ordered=False)
)

    # Plot
    sns.scatterplot(
        data=rel_std_and_se_data_ml,
        x='rel_se',
        y='rel_std',
        hue='reliability_category',
        palette=reliability_palette,
        ax=axs[i],
        s=450,
        alpha=0.75
    )
    axs[i].set_title(f'{param} – Clustered')
    axs[i].set_xlabel('Rel. SE')
    axs[i].set_ylabel('Rel. Std')

    # To store the final dataframe of rel_std and rel_se values with added columns including reliability category labels obtained from KMeans analysis
    results_per_param[param] = rel_std_and_se_data_ml.copy()

    # Manual thresholding
    std_thresh_manual = rel_std_and_se_data_ml['rel_std'].median()
    se_thresh_manual = rel_std_and_se_data_ml['rel_se'].median()

    rel_std_and_se_data_manual = rel_std_and_se_data_ml.copy()
    rel_std_and_se_data_manual['manual_reliability_category'] = rel_std_and_se_data_manual.apply(lambda row: classify_cluster_center(row['rel_std'], row['rel_se'], std_thresh_manual, se_thresh_manual), axis=1)

    # Group once to avoid repeated computation
    grouped_manual = rel_std_and_se_data_manual.groupby('manual_reliability_category')

    for manual_reliability_label, group_df_manual in grouped_manual:
      count_manual = len(group_df_manual)
      print(f"\nParameter: {param} – Cluster: '{manual_reliability_label}' → Total count: {count_manual}")

      method_counts_manual = group_df_manual['method'].value_counts(normalize=True).sort_values(ascending=False)
      print("Method distribution within cluster for manual categorization:")
      for method, pct in method_counts_manual.items():
        print(f"  {method}: {pct:.2%}")

    # First group entire dataset by method and reliability. unstack() is used here which takes the second level of grouped index (i.e. reliability_category entries) and converts it to columns while maintaining method as the row index
    method_reliability_counts_manual = rel_std_and_se_data_manual.groupby(['method', 'manual_reliability_category']).size().unstack(fill_value=0)

    # Ensure all expected reliability categories are present
    for label in rel_std_and_se_data_manual['manual_reliability_category']:
      if label not in method_reliability_counts_manual.columns:
        method_reliability_counts_manual[label] = 0  # add missing columns if needed

    # Print ratio info
    print(f"\nParameter: {param} – Method-wise reliability ratios for Manual Categorization:")
    for method, row in method_reliability_counts_manual.iterrows():
        high = row.get('High reliability', 0)
        low = row.get('Low reliability', 0)
        medium = row.get('Medium reliability (low variability, high uncertainty)', 0) + \
             row.get('Medium reliability (high variability, low uncertainty)', 0)

        if high > 0:
            low_to_high = low / high
            med_to_high = medium / high
            print(f"  {method}: Low/High = {low_to_high:.4f}, Medium/High = {med_to_high:.4f}")
        else:
            print(f"  {method}: High reliability count is 0 → ratios undefined")

    manual_results_per_param[param] = rel_std_and_se_data_manual.copy()

    rel_std_and_se_data_manual['manual_reliability_category'] = rel_std_and_se_data_manual['manual_reliability_category'].astype(
        CategoricalDtype(categories=all_categories, ordered=False)
    )

    sns.scatterplot(
        data=rel_std_and_se_data_manual,
        x='rel_se',
        y='rel_std',
        hue='manual_reliability_category',
        ax=axs_manual[i],
        palette=reliability_palette,
        s=450,
        alpha=0.75
    )

    axs_manual[i].scatter(std_thresh_manual, se_thresh_manual,
        color='black', marker='x', s=300, label='Median Thresholds')

    axs_manual[i].set_title(f'{param} – Manual Categorization')
    axs_manual[i].set_xlabel('Rel. SE')
    axs_manual[i].set_ylabel('Rel. Std')

# Remove unused axes
for j in range(i + 1, len(axs)):
    fig.delaxes(axs[j])

plt.tight_layout()
plt.show()
